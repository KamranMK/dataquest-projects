{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "\n",
    "In this project, we will build a spam filter with Naive Bayes. We're going to study the practical side of the Naive Bayes algorithm by building a spam filter for SMS messages.\n",
    "\n",
    "To classify messages as spam or non-spam, the computer would need to follow the below logic:\n",
    "\n",
    "* Learns how humans classify messages.\n",
    "* Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "* Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "So our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the The [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). You can also download the dataset directly [from this link](https://dq-content.s3.amazonaws.com/433/SMSSpamCollection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers.\n",
    "\n",
    "Let's start by reading in the dataset. You'll be able to find the solutions to this project at this link or by clicking the key icon at the top right of the interface.\n",
    "\n",
    "**Disclaimer:** This project is part of series of guided projects on dataquest. All of the code and logic belongs to me with [dataquest](dataquest.io) providing guidance on building the spam filter. The project is built as a result of going through the coded classes on the platform. Formulas and content is taken from the guided project.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "Let's read in the dataset and try to get acquainted with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the dataset with no header\n",
    "data = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check few rows \n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 5572 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "# columns and rows in the dataset\n",
    "print(\"Dataset contains {} rows and {} columns\".format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is somewhat inbalanced since we have 86,5% of ham and only 13,4% of spam messages. Nevertheless, let's move to preparing our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "Let's start by splitting our data in training and test set. We will first randomize the entire dataset to ensure that ham and spam messages are spread properly throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomize the entire dataset\n",
    "data = data.sample(frac=1, random_state=1)\n",
    "\n",
    "# split the data to train and test\n",
    "training_data, testing_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865997\n",
       "spam    0.134003\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check percentage of ham and spam in training set\n",
    "training_data.Label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the proprtion of ham and spam in the training set is similar to the original dataset.\n",
    "\n",
    "### Reminder on Naive Bayes\n",
    "\n",
    "Let's recall from our previous classes and missions that when a new message comes in, our Naive Bayes algorithm will make the classification based on the results it gets to these two equations (note that we replaced \"Spam$^C$\" with \"Ham\" inside the second equation below):\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, recall that we need to use these equations:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "Let's also summarize what the terms in the equations above mean:\n",
    "\n",
    "\\begin{aligned}\n",
    "&N_{w_i|Spam} = \\text{the number of times the word } w_i \\text{ occurs in spam messages} \\\\\n",
    "&N_{w_i|Spam^C} = \\text{the number of times the word } w_i \\text{ occurs in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Spam} = \\text{total number of words in spam messages} \\\\\n",
    "&N_{Spam^C} = \\text{total number of words in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Vocabulary} = \\text{total number of words in the vocabulary} \\\\\n",
    "&\\alpha = 1 \\ \\ \\ \\ (\\alpha \\text{ is a smoothing parameter})\n",
    "\\end{aligned}\n",
    "\n",
    "To calculate probabilities we will need to transform our dataset to a format where can calculate them. Basically each word in the messages will become a feature with each row representing the counts of occurence of that word in the specific row/message. We will transform the SMS column to vector of words.\n",
    "\n",
    "Let's transform the SMS column to vector of words and relevant counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>ham</td>\n",
       "      <td>i sent you the prices and do you mean the   lt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>ham</td>\n",
       "      <td>house maid is the murderer  coz the man was mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>ham</td>\n",
       "      <td>sad story of a man   last week was my b day  m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3482</th>\n",
       "      <td>ham</td>\n",
       "      <td>wherre s my boytoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>ham</td>\n",
       "      <td>fyi i m gonna call you sporadically starting a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS\n",
       "2739   ham  i sent you the prices and do you mean the   lt...\n",
       "4437   ham  house maid is the murderer  coz the man was mu...\n",
       "2259   ham  sad story of a man   last week was my b day  m...\n",
       "3482   ham                           wherre s my boytoy      \n",
       "4067   ham  fyi i m gonna call you sporadically starting a..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make a copy of our training data to avoid SettingWithCopyWarning\n",
    "training_set = training_data.copy(deep=True)\n",
    "\n",
    "# lower string and remove punctuation and numbers from SMS column\n",
    "training_set['SMS'] = training_data['SMS'].str.replace('[^a-zA-Z]', ' ').str.lower()\n",
    "\n",
    "# check results\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a vocabulary for the messages in the training set\n",
    "\n",
    "# get the list of all words\n",
    "all_words = [word for lst in training_set['SMS'].str.split().values for word in lst]\n",
    "\n",
    "# get the vocabulary\n",
    "vocabulary = list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up a dictionary with words as keys and counts as columns\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "# loop through and populate the dictionary with counts\n",
    "for index, sms in enumerate(training_set['SMS'].str.split()):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert dictionary to dataframe\n",
    "sms_word_vectors = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "# reset index for training set\n",
    "training_set.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# concatenate the dataframe with traininng data\n",
    "training_data = pd.concat([sms_word_vectors, training_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniye</th>\n",
       "      <th>aathi</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abel</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abi</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>zf</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zs</th>\n",
       "      <th>zyada</th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>i sent you the prices and do you mean the   lt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>house maid is the murderer  coz the man was mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>sad story of a man   last week was my b day  m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6748 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  aa  aah  aaniye  aathi  abbey  abel  aberdeen  abi  ability  \\\n",
       "0  0   0    0       0      0      0     0         0    0        0   \n",
       "1  0   0    0       0      0      0     0         0    0        0   \n",
       "2  1   0    0       0      0      0     0         0    0        0   \n",
       "\n",
       "                         ...                          zf  zhong  zindgi  zoe  \\\n",
       "0                        ...                           0      0       0    0   \n",
       "1                        ...                           0      0       0    0   \n",
       "2                        ...                           0      0       0    0   \n",
       "\n",
       "   zogtorius  zoom  zs  zyada  Label  \\\n",
       "0          0     0   0      0    ham   \n",
       "1          0     0   0      0    ham   \n",
       "2          0     0   0      0    ham   \n",
       "\n",
       "                                                 SMS  \n",
       "0  i sent you the prices and do you mean the   lt...  \n",
       "1  house maid is the murderer  coz the man was mu...  \n",
       "2  sad story of a man   last week was my b day  m...  \n",
       "\n",
       "[3 rows x 6748 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check our final training data\n",
    "training_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcualting Constants First\n",
    "\n",
    "Now that we have the word vector representation of each of our messages. Let's recall that the Naive Bayes algorithm will need to know the probability values of the two equations below to be able to classify new messages:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam) \\\\\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam) \\\\\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, recall that we need to use these equations:\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} \\\\\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} \\\\\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "We will also need to introduce a parameter $\\alpha$ which essentially prevents division by 0 where probabilities are 0, so called Laplace Smoothing.\n",
    "\n",
    "Let's first calculate:\n",
    "* P(Spam) and P(Ham)\n",
    "* $N_{spam}$, $N_{ham}$, $N_{vocabulary}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize an alpha paramter\n",
    "alpha = 1\n",
    "\n",
    "# calculate overall probabilities\n",
    "p_spam = training_data['Label'].value_counts(normalize=True)[1]\n",
    "p_ham = training_data['Label'].value_counts(normalize=True)[0]\n",
    "\n",
    "# calculate counts of words in spam, ham and vocab\n",
    "n_spam = training_data.loc[training_data.Label == 'spam', training_data.columns.difference(['SMS', 'Label'])].sum().sum()\n",
    "n_ham = training_data.loc[training_data.Label == 'ham', training_data.columns.difference(['SMS', 'Label'])].sum().sum()\n",
    "n_vocab = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Parameters\n",
    "\n",
    "Now, we will use our transformed training data to calculate the probabilities for each word ocurring given the label of the message. For instance, let's say we receive two new messages:\n",
    "* \"secret code\"\n",
    "* \"secret party 2night\"\n",
    "\n",
    "We'll need to calculate P(\"secret\"|Spam) for both these messages, and we can use the training set to get the values we need to find a result for the equation below:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\text{\"secret\"}|Spam) = \\frac{N_{\"secret\"|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "We will perform the calculation for both types of messages, spam and ham. A generalized formula for our case would be like the following:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} \\\\\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize for each type of message a dictionary(vocabulary) with key being words from vocabulary, and values set to 0\n",
    "spam_words = {key: 0 for key in vocabulary}\n",
    "ham_words = {key: 0 for key in vocabulary}\n",
    "\n",
    "# isolate spam and ham messages from our training data\n",
    "spam_training_set = training_data[training_data['Label'] == 'spam']\n",
    "ham_training_set = training_data[training_data['Label'] == 'ham']\n",
    "\n",
    "# iterate over vocabularies of spam and ham words and calculate probabilities using formulas above\n",
    "for word in spam_words:\n",
    "    word_count = spam_training_set[str(word)].sum()\n",
    "    word_prob = (word_count + alpha) / (n_spam + alpha * n_vocab)\n",
    "    spam_words[word] = word_prob\n",
    "\n",
    "for word in ham_words:\n",
    "    word_count = ham_training_set[word].sum()\n",
    "    word_prob = (word_count + alpha) / (n_ham + alpha * n_vocab)\n",
    "    ham_words[word] = word_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying a new message\n",
    "\n",
    "Now that we've calculated all the constants and parameters we need, we can start creating the spam filter. The filter can be understood as \n",
    "\n",
    "* a function that takes as an input list of words\n",
    "* calculates the probability of spam or ham given the list of words\n",
    "* compares the values of those probabilities\n",
    "* returns the label for which the probability is higher\n",
    "    * in case of equality, then human intervention would be needed to classify the message\n",
    "    \n",
    "To write the function we need to use these 2 equations:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to classify the message as spam or ham\n",
    "def classify(message: str):\n",
    "\n",
    "    # perform clean up of input message and split to words (tokenization)\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "    \n",
    "    # calculate probability of spam/ham given message\n",
    "    p_spam_given_message = p_spam * np.prod([spam_words[w] for w in message if w in spam_words])\n",
    "    p_ham_given_message = p_ham * np.prod([ham_words[w] for w in message if w in ham_words])\n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our classifier with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying message 1\n",
      "P(Spam|message): 2.9659964898116258e-25\n",
      "P(Ham|message): 3.0485737687547568e-27\n",
      "Label: Spam\n",
      "\n",
      "\n",
      "Classifying message 2\n",
      "P(Spam|message): 8.288309333372596e-25\n",
      "P(Ham|message): 4.969856371695636e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "test_message1 = 'WINNER!! This is the secret code to unlock the money: C3421.'\n",
    "test_message2 = \"Sounds good, Tom, then see u there\"\n",
    "\n",
    "print(\"Classifying message 1\")\n",
    "classify(test_message1)\n",
    "print(\"\\n\")\n",
    "print(\"Classifying message 2\")\n",
    "classify(test_message2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our classifier was able to correctly classify these two test messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuing the Spam Filter's Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use our test set, prepared earlier to test the accuracy of our spam filter. For that we need to slightly modify our initial classify function to output spam and ham instead of printing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to classify the message as spam or ham\n",
    "def classify_test_set(message: str):\n",
    "\n",
    "    # perform clean up of input message and split to words (tokenization)\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "    \n",
    "    # calculate probability of spam/ham given message\n",
    "    p_spam_given_message = p_spam * np.prod([spam_words[w] for w in message if w in spam_words])\n",
    "    p_ham_given_message = p_ham * np.prod([ham_words[w] for w in message if w in ham_words])\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function that returns labels instead of printing them, we can use it to create a new column in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataquest/system/env/python3/lib/python3.4/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>ham</td>\n",
       "      <td>You call times job today ok umma and ask them ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>ham</td>\n",
       "      <td>Argh why the fuck is nobody in town ;_;</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>ham</td>\n",
       "      <td>Good Morning my Dear........... Have a great &amp;...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>ham</td>\n",
       "      <td>He's really into skateboarding now despite the...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>ham</td>\n",
       "      <td>Purity of friendship between two is not about ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS predicted\n",
       "2605   ham  You call times job today ok umma and ask them ...       ham\n",
       "5114   ham            Argh why the fuck is nobody in town ;_;       ham\n",
       "3243   ham  Good Morning my Dear........... Have a great &...       ham\n",
       "3641   ham  He's really into skateboarding now despite the...       ham\n",
       "2799   ham  Purity of friendship between two is not about ...       ham"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data['predicted'] = testing_data['SMS'].apply(classify_test_set)\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the predicted values with the actual values to measure how good our spam filter is with classifying new messages. To make the measurement, we'll use accuracy as a metric:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{number of correctly classified messages}}{\\text{total number of classified messages}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our classifier is 98.35%\n"
     ]
    }
   ],
   "source": [
    "# count the number of correctly classified vs total classified\n",
    "correctly_classified = testing_data[testing_data.Label == testing_data.predicted].shape[0]\n",
    "total_classifier = testing_data.shape[0]\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = correctly_classified / total_classifier\n",
    "print(\"Accuracy of our classifier is {:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Further Ideas\n",
    "\n",
    "In this project, we managed to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The filter had an accuracy of 98.35% on the test set, which is an excellent result.\n",
    "\n",
    "We can keep working on the classifier to improve it. Here are some ideas that we could take and try to improve the classifier.\n",
    "\n",
    "* Isolate the 14 messages that were classified incorrectly and try to figure out why the algorithm reached the wrong conclusions.\n",
    "* Make the filtering process more complex by making the algorithm sensitive to letter case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
